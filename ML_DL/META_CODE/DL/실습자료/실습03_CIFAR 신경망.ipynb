{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"실습4. CIFAR 신경망.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6oE6JgovTZ4E"},"source":["# Setting"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mckiYKQqATiy","executionInfo":{"status":"ok","timestamp":1633140159716,"user_tz":-540,"elapsed":603,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","\n","\n","from torchsummary import summary\n","from tqdm import tqdm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gG7tUAGUCWn","executionInfo":{"status":"ok","timestamp":1633140161089,"user_tz":-540,"elapsed":206,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["# Global Variable For training\n","# You just use the following hyper-parameters\n","BATCH_SIZE = 32\n","NUM_EPOCH = 2\n","LEARNING_RATE = 0.01\n","CRITERION = nn.CrossEntropyLoss()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXmD4MyRATiz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633140165294,"user_tz":-540,"elapsed":1728,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"50a388d5-d6c4-4c53-9cc3-704a18d6a963"},"source":["# CIFAR10 Dataset\n","train_dataset = dsets.CIFAR10(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = dsets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor())\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4S96yn1BsK3F","executionInfo":{"status":"ok","timestamp":1633140169232,"user_tz":-540,"elapsed":231,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"2e5e53e7-e957-4c95-8a66-ed273ea2c718"},"source":["train_dataset.class_to_idx"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'airplane': 0,\n"," 'automobile': 1,\n"," 'bird': 2,\n"," 'cat': 3,\n"," 'deer': 4,\n"," 'dog': 5,\n"," 'frog': 6,\n"," 'horse': 7,\n"," 'ship': 8,\n"," 'truck': 9}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"eYYRRYnLr-xY","executionInfo":{"status":"ok","timestamp":1633140169955,"user_tz":-540,"elapsed":364,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"b3d99536-ae7f-4bbb-dd15-15f386449e54"},"source":["index = 6\n","print(train_dataset.targets[index])\n","print(train_dataset.data[index].shape)\n","plt.imshow(train_dataset.data[index])"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","(32, 32, 3)\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f551856c1d0>"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"VTJ94ze1ATiz","executionInfo":{"status":"ok","timestamp":1633140172559,"user_tz":-540,"elapsed":204,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["def fit(model,train_loader):\n","   # 1epoch training\n","    model.train()\n","    device = next(model.parameters()).device.index\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","    losses = []\n","    for i, data in enumerate(train_loader):\n","        image = data[0].type(torch.FloatTensor).cuda()\n","        label = data[1].type(torch.LongTensor).cuda()\n","\n","        pred_label = model(image)\n","        loss = CRITERION(pred_label, label)\n","        losses.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    avg_loss = sum(losses)/len(losses)\n","    return avg_loss"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lB99radATiz","executionInfo":{"status":"ok","timestamp":1633140174530,"user_tz":-540,"elapsed":201,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["def eval(model, test_loader):\n","    model.eval()\n","    device = next(model.parameters()).device.index\n","    pred_labels = []\n","    real_labels = []\n","\n","    for i, data in enumerate(test_loader):\n","        image = data[0].type(torch.FloatTensor).cuda(device)\n","        label = data[1].type(torch.LongTensor).cuda(device)\n","        real_labels += list(label.cpu().detach().numpy())\n","        \n","        pred_label = model(image)\n","        pred_label = list(pred_label.cpu().detach().numpy())\n","        pred_labels += pred_label\n","        \n","    real_labels = np.array(real_labels)\n","    pred_labels = np.array(pred_labels)\n","    pred_labels = pred_labels.argmax(axis=1)\n","    acc = sum(real_labels==pred_labels)/len(real_labels)*100\n","    \n","    return acc"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IF2eqtw0ATiz"},"source":["# Task1: Compare between simple MLP and simple CNN"]},{"cell_type":"markdown","metadata":{"id":"-h2bsAhSATi0"},"source":["### MLP"]},{"cell_type":"code","metadata":{"id":"fioBny6wW_fO"},"source":["class SimpleMLP(nn.Module):\n","    def __init__(self):\n","        super(SimpleMLP, self).__init__()\n","        # Fully-connected layer\n","        self.fc1_1 = nn.Linear(3*32*32, 8*28*28)\n","        self.act1_1 = nn.ReLU()\n","        self.fc1_2 = nn.Linear(8*28*28, 8*24*24)\n","        self.act1_2 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(2)\n","        \n","        self.fc2_1 = nn.Linear(8*12*12, 16*8*8)\n","        self.act2_1 = nn.ReLU()\n","        self.fc2_2 = nn.Linear(16*8*8, 16*4*4)\n","        self.act2_2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        # Output layer\n","        self.out = nn.Linear(16*2*2, 10)\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 3*32*32)\n","        \n","        x = self.act1_1(self.fc1_1(x))\n","        x = self.act1_2(self.fc1_2(x))\n","        x = x.view(-1, 8, 24, 24)\n","        x = self.pool1(x)\n","        x = x.view(-1, 8*12*12)\n","        \n","        x = self.act2_1(self.fc2_1(x))\n","        x = self.act2_2(self.fc2_2(x))\n","        x = x.view(-1, 16, 4, 4)\n","        x = self.pool2(x)\n","        x = x.view(-1, 16*2*2)\n","\n","        out = self.out(x)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-HZj2KqATi0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630808225191,"user_tz":-540,"elapsed":53329,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"077fe161-120c-478a-f66b-5a361d31b125"},"source":["mlp_model = SimpleMLP().cuda()\n","train_loss1 = []\n","test_accuracy1 = []\n","for epoch in tqdm(range(NUM_EPOCH)):\n","    train_loss1.append(fit(mlp_model, train_loader))\n","    test_accuracy1.append(eval(mlp_model, test_loader))\n","summary(mlp_model, input_size = (3,32,32))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","100%|██████████| 2/2 [00:43<00:00, 21.58s/it]"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 6272]      19,273,856\n","              ReLU-2                 [-1, 6272]               0\n","            Linear-3                 [-1, 4608]      28,905,984\n","              ReLU-4                 [-1, 4608]               0\n","         MaxPool2d-5            [-1, 8, 12, 12]               0\n","            Linear-6                 [-1, 1024]       1,180,672\n","              ReLU-7                 [-1, 1024]               0\n","            Linear-8                  [-1, 256]         262,400\n","              ReLU-9                  [-1, 256]               0\n","        MaxPool2d-10             [-1, 16, 2, 2]               0\n","           Linear-11                   [-1, 10]             650\n","================================================================\n","Total params: 49,623,562\n","Trainable params: 49,623,562\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 0.19\n","Params size (MB): 189.30\n","Estimated Total Size (MB): 189.51\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"QIeK_TL3v1fW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630808278406,"user_tz":-540,"elapsed":200,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"da343882-2619-4a78-e4e6-f6cb312913de"},"source":["train_loss1, test_accuracy1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([1.95331401014328, 1.6743597331047058], [36.63, 43.3])"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"eUTF0MIAATi0"},"source":["### CNN"]},{"cell_type":"markdown","metadata":{"id":"rcxjFbQiATi1"},"source":["# Task 2: VGG\n","\n"]},{"cell_type":"markdown","metadata":{"id":"T0ym7Go2ATi1"},"source":["### VGG"]},{"cell_type":"code","metadata":{"id":"L6UktVlYb4Ip"},"source":["import torch.utils.model_zoo as model_zoo\n","\n","model_urls = {\n","    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n","    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n","    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n","    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n","    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n","    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n","    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n","    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqQ8jv2vATi1"},"source":["class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=1000, init_weights=False):\n","        super(VGG, self).__init__()\n","        self.features = features # feature extractor : convolutional filter의 뭉치. nn.conv2d, nn.batchnorm, .... nn.linear\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 1 * 1, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x) # batch size x 512 x 1 x 1\n","        x = x.view(x.size(0), -1) # x.size(0)=batch size * (512*1*1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(\n","                    m.weight, mode='fan_out', nonlinearity='relu') # kaiming initialization\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","def make_layers(cfg, batch_norm=False): # config\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == 'M':\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            if batch_norm:\n","                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","            else:\n","                layers += [conv2d, nn.ReLU(inplace=True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","\n","cfg = {\n","    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","def vgg11(pretrained=False, **kwargs):\n","    \"\"\"VGG 11-layer model (configuration \"A\")\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['A']), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))\n","    return model\n","\n","\n","def vgg11_bn(pretrained=False, **kwargs):\n","    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))\n","    return model\n","\n","\n","def vgg13(pretrained=False, **kwargs):\n","    \"\"\"VGG 13-layer model (configuration \"B\")\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['B']), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg13']))\n","    return model\n","\n","\n","def vgg13_bn(pretrained=False, **kwargs):\n","    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['B'], batch_norm=True), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg13_bn']))\n","    return model\n","\n","\n","def vgg16(pretrained=False, **kwargs):\n","    \"\"\"VGG 16-layer model (configuration \"D\")\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['D']), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg16']))\n","    return model\n","\n","\n","def vgg16_bn(pretrained=False, **kwargs):\n","    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))\n","    return model\n","\n","\n","def vgg19(pretrained=False, **kwargs):\n","    \"\"\"VGG 19-layer model (configuration \"E\")\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['E']), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))\n","    return model\n","\n","\n","def vgg19_bn(pretrained=False, **kwargs):\n","    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","    \"\"\"\n","    if pretrained:\n","        kwargs['init_weights'] = False\n","    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)\n","    if pretrained:\n","        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"db77_zN-ATi1","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1631843938680,"user_tz":-540,"elapsed":84185,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"6b64bd3a-0e47-4a82-844e-94f471366b4b"},"source":["vgg_model = vgg11_bn(num_classes=10).cuda()\n","train_loss3 = []\n","test_accuracy3 = []\n","for epoch in tqdm(range(NUM_EPOCH)):\n","    train_loss3.append(fit(vgg_model, train_loader))\n","    test_accuracy3.append(eval(vgg_model, test_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/2 [01:23<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-241624cf572a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_accuracy3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest_accuracy3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-bf82cabb5ad7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1xGLzcKaDI9","executionInfo":{"status":"ok","timestamp":1631844003237,"user_tz":-540,"elapsed":229,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"4b7d98ed-6eb8-4ec7-af94-15bc13b5f4eb"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Sep 17 02:00:03 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    57W / 149W |   2143MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"mkSj4fJAATi1"},"source":["### Task3 : ResNet"]},{"cell_type":"code","metadata":{"id":"CJZ5r_EZATi2"},"source":["'''ResNet in PyTorch.\n","For Pre-activation ResNet, see 'preact_resnet.py'.\n","Reference:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4) # batch size x 512 x 1 x 1\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3, 4, 6, 3])\n","\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3, 4, 23, 3])\n","\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3, 8, 36, 3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1, 3, 32, 32))\n","    print(y.size())\n","\n","# test()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvILm-PKATi2","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1631844816782,"user_tz":-540,"elapsed":130995,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"c25c9eb1-b0f8-49ee-bd13-89ceeeb4ea29"},"source":["resnet_model = ResNet18().cuda()\n","train_loss4 = []\n","test_accuracy4 = []\n","for epoch in tqdm(range(NUM_EPOCH)):\n","    train_loss4.append(fit(resnet_model, train_loader))\n","    test_accuracy4.append(eval(resnet_model, test_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/2 [02:10<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-dac1d9f6b44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_accuracy4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest_accuracy4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-bf82cabb5ad7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRITERION\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"dXJtIfsuATi2"},"source":["# Task 4 : MobileNet\n"]},{"cell_type":"code","metadata":{"id":"liPQoqBjbpKh","executionInfo":{"status":"ok","timestamp":1633140241468,"user_tz":-540,"elapsed":223,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["from torchvision.models.utils import load_state_dict_from_url"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"vp5iEW1ibhEJ","executionInfo":{"status":"ok","timestamp":1633140413594,"user_tz":-540,"elapsed":233,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["class ConvBNReLU(nn.Sequential):\n","    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1, bn_aff = True):\n","        padding = (kernel_size - 1) // 2\n","        super(ConvBNReLU, self).__init__(\n","            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n","            nn.BatchNorm2d(out_planes, affine = bn_aff),\n","            nn.ReLU6(inplace=True)\n","        )"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-oZpQvMbj9M","executionInfo":{"status":"ok","timestamp":1633141233749,"user_tz":-540,"elapsed":238,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["# pointwise - BN - relu - depthwise - bn - relu - pointwise - bn\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(self, inp, oup, stride, expand_ratio, shortcut, bn_aff):\n","        super(InvertedResidual, self).__init__()\n","        self.shortcut = shortcut\n","        self.bn_aff = bn_aff\n","        self.stride = stride\n","        assert stride in [1, 2]\n","\n","        hidden_dim = int(round(inp * expand_ratio))\n","        self.use_res_connect = self.stride == 1 and inp == oup\n","\n","        layers = []\n","        if expand_ratio != 1:\n","            # pw\n","            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, bn_aff = self.bn_aff))\n","        layers.extend([\n","            # dw : feature map의 out channels는 hidden_dim\n","            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, bn_aff = self.bn_aff),\n","            # pw-linear squeeze #kernel size - stride ,- padding\n","            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","            nn.BatchNorm2d(oup, affine = self.bn_aff),\n","        ])\n","        self.conv = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        if self.use_res_connect:\n","            if self.shortcut:\n","                return x + self.conv(x)\n","            else:\n","                return self.conv(x)\n","        else:\n","            return self.conv(x)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"RymAAEzlblXR","executionInfo":{"status":"ok","timestamp":1633141236279,"user_tz":-540,"elapsed":211,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["model_urls = {\n","    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n","}\n","def _make_divisible(x, divisible_by=8):\n","    import numpy as np\n","    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n","\n","class MobileNetV2(nn.Module):\n","    def __init__(self,\n","                 num_classes=1000,\n","                 width_mult=1.0,\n","                 inverted_residual_setting=None,\n","                 round_nearest=8,\n","                 block=None,\n","                 shortcut=True,\n","                 bn_aff=True):\n","        \"\"\"\n","        MobileNet V2 main class\n","\n","        Args:\n","            num_classes (int): Number of classes\n","            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n","            inverted_residual_setting: Network structure\n","            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n","            Set to 1 to turn off rounding\n","            block: Module specifying inverted residual building block for mobilenet\n","\n","        \"\"\"\n","        super(MobileNetV2, self).__init__()\n","\n","        if block is None:\n","            block = InvertedResidual\n","        input_channel = 32\n","        last_channel = 1280\n","        self.bn_aff = bn_aff\n","        self.shortcut = shortcut\n","\n","        if inverted_residual_setting is None:\n","            inverted_residual_setting = [\n","                # t, c, n, s\n","                [1, 16, 1, 1],\n","                [6, 24, 2, 2], # q1) stride 2가 두번 반복되냐? q2)[16 - 96 - 24] - [16 - 96 - 24] #labor intensive\n","                [6, 32, 3, 2],\n","                [6, 64, 4, 2],\n","                [6, 96, 3, 1],\n","                [6, 160, 3, 2],\n","                [6, 320, 1, 1],\n","            ]\n","\n","        # only check the first element, assuming user knows t,c,n,s are required\n","        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n","            raise ValueError(\"inverted_residual_setting should be non-empty \"\n","                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n","\n","        # building first layer\n","        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n","        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n","        features = [ConvBNReLU(3, input_channel, stride=2, bn_aff = self.bn_aff)]\n","        \n","        \n","        # building inverted residual blocks\n","        for t, c, n, s in inverted_residual_setting:\n","            output_channel = _make_divisible(c * width_mult, round_nearest)\n","            for i in range(n):\n","                stride = s if i == 0 else 1\n","                features.append(block(input_channel, output_channel, stride, expand_ratio=t, shortcut = self.shortcut, bn_aff = self.bn_aff))\n","                input_channel = output_channel\n","                \n","                \n","        # building last several layers\n","        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, bn_aff = self.bn_aff))\n","        \n","        # make it nn.Sequential\n","        self.features = nn.Sequential(*features)\n","\n","        # building classifier\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.Linear(self.last_channel, num_classes),\n","        )\n","\n","        # weight initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.ones_(m.weight)\n","                nn.init.zeros_(m.bias)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.zeros_(m.bias)\n","\n","    def _forward_impl(self, x):\n","        # This exists since TorchScript doesn't support inheritance, so the superclass method\n","        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n","        x = self.features(x)\n","        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]\n","        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LO5xCq7ebnZP","executionInfo":{"status":"ok","timestamp":1633141252932,"user_tz":-540,"elapsed":208,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}}},"source":["def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n","    \"\"\"\n","    Constructs a MobileNetV2 architecture from\n","    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" <https://arxiv.org/abs/1801.04381>`_.\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","    \"\"\"\n","    model = MobileNetV2(**kwargs)\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n","                                              progress=progress)\n","        model.load_state_dict(state_dict)\n","    return model"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"Orl-Jfz6ATi2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633141405406,"user_tz":-540,"elapsed":128129,"user":{"displayName":"김태현","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01897584896270801478"}},"outputId":"9f33b438-f0a5-4852-cbea-6501aab68dc6"},"source":["mobile_model = mobilenet_v2(num_classes=10).cuda()\n","train_loss5 = []\n","test_accuracy5 = []\n","for epoch in tqdm(range(NUM_EPOCH)):\n","    train_loss5.append(fit(mobile_model, train_loader))\n","    test_accuracy5.append(eval(mobile_model, test_loader))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [02:05<00:00, 62.83s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"eHYVMUN2ukyb"},"source":[""],"execution_count":null,"outputs":[]}]}