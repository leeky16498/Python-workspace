{"cells":[{"cell_type":"markdown","metadata":{"id":"7vYnhSvRGeLo"},"source":["# 기본 신경망 학습\n","\n","Pytorch를 사용해 신경망을 학습하는 것을 익히기 위한 자료입니다."]},{"cell_type":"markdown","metadata":{"id":"VCbF1ob7GeLs"},"source":["## MNIST 데이터 학습을 위한 신경망 학습"]},{"cell_type":"markdown","metadata":{"id":"htrcBLARGmUR"},"source":["MNIST는 손글씨의 숫자(0~9)를 분류하는 문제를 위한 데이터입니다.\n","\n","각 사진 데이터는 0~9의 데이터를 지니고 있습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"1aHMKah2GeLs"},"source":["torch와 관련된 library의 import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8uMi3Pp1GeLt"},"outputs":[],"source":["import torch \n","import torch.nn as nn #\n","import torch.nn.functional as F # various activation functions for model\n","import torchvision # You can load various Pretrained Model from this package \n","import torchvision.datasets as vision_dsets\n","import torchvision.transforms as T # Transformation functions to manipulate images\n","import torch.optim as optim # various optimization functions for model\n","\n","\n","from torch.autograd import Variable \n","from torch.utils import data"]},{"cell_type":"markdown","metadata":{"id":"6IQGfWUiGeLu"},"source":["데이터 분석을 위한 library의 import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmtVMH7rGeLu"},"outputs":[],"source":["import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import itertools"]},{"cell_type":"markdown","metadata":{"id":"Odo2iFINGeLv"},"source":["## Confusion Matrix (오차행렬)\n","\n","### Confusion matrix란?\n","- Training을 통한 Prediction 성능을 예측 value와 실제 value를 비교하기 위한 표\n","\n","![image.png](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2\u0026fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fx3c4F%2FbtqDn2lU1Ab%2FVAxPJQcU0rYgnxmZlTnYdk%2Fimg.png\n",")\n","https://shinminyong.tistory.com/m/28 의 설명을 빌려와 설명하겠습니다.\n","\n","### 자세한 설명\n","\n","먼저 표 내부에 있는 단어들에 대한 설명을 드리겠습니다.(설명의 편의성을 위해서 환자다 / 환자가 아니다 로 설명드리겠습니다.)\n","\n","Actual(Positive) : 실제 환자\n","\n","Actual(Negative) : 실제 환자가 아님\n","\n","Predict(Positive) : 실제 환자로 예측\n","\n","Predict(Negative) : 환자가 아닌 것으로 예측\n","\n","\n","이번엔 각각의 교차된 지표에 대해서 설명드리겠습니다. 이 부분은 앞의 단어를 성공/실패 뒤의 단어를 긍정예측/부정예측 이라고 생각하시면 생각하시기 쉬울겁니다!\n","\n","TP(True Positive) : 긍정예측을 성공 즉, 환자라고 예측해서 실제 환자임을 맞춤\n","\n","TN(True Negative) : 부정예측을 성공 즉, 비환자라고 예측하여 실제 비환자임을 맞춤\n","\n","FP(False Positive) : 긍정예측을 실패 즉, 환자라고 예측했지만 비환자임\n","\n","FN(False Negative) : 부정예측을 실패 즉, 비환자라고 예측했지만 실제 환자임"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAtszE8cGeLx"},"outputs":[],"source":["def plot_confusion_matrix(cm, target_names=None, cmap=None, normalize=True, labels=True, title='Confusion matrix'):\n","    accuracy = np.trace(cm) / float(np.sum(cm))\n","    misclass = 1 - accuracy\n","\n","    if cmap is None:\n","        cmap = plt.get_cmap('Blues')\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        \n","    plt.figure(figsize=(20, 15))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","\n","    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n","    \n","    if target_names is not None:\n","        tick_marks = np.arange(len(target_names))\n","        plt.xticks(tick_marks, target_names)\n","        plt.yticks(tick_marks, target_names)\n","    \n","    if labels:\n","        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","            if normalize:\n","                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n","                         horizontalalignment=\"center\",\n","                         color=\"white\" if cm[i, j] \u003e thresh else \"black\")\n","            else:\n","                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n","                         horizontalalignment=\"center\",\n","                         color=\"white\" if cm[i, j] \u003e thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"igehYlNTGeLy"},"source":["## DataLoader의 생성\n","\n","MNIST Data의 경우 Pytorch의 torchvision에 이미 있는 데이터 셋입니다. 그리하여 torchvision으로부터 dataset을 받아오고, 받아온 데이터 셋을 원하는 batch size와 transform함수를 통해 data loader로 아래 처럼 만들어줍니다.\n","\n","![mnist](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n","\n","더욱 자세한 설명은 https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader 을 참고하시면 됩니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-U7hrTeVGeLy"},"outputs":[],"source":["def MNIST_DATA(root='./data',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n","    print (\"[+] Get the MNIST DATA\")\n","    \"\"\"\n","    We will use Mnist data for our tutorial \n","    \"\"\"\n","    mnist_train = vision_dsets.MNIST(root = root,  #root is the place to store your data. \n","                                    train = True,  \n","                                    transform = T.ToTensor(), # convert data to tensor \n","                                    download = True)  # whether to download the data\n","    mnist_test = vision_dsets.MNIST(root = root,\n","                                    train = False, \n","                                    transform = T.ToTensor(),\n","                                    download = True)\n","    \"\"\"\n","    Data Loader is a iterator that fetches the data with the number of desired batch size. \n","    * Practical Guide : What is the optimal batch size? \n","      - Usually.., higher the batter. \n","      - We recommend to use it as a multiple of 2 to efficiently utilize the gpu memory. (related to bit size)\n","    \"\"\"\n","    trainDataLoader = data.DataLoader(dataset = mnist_train,  # information about your data type\n","                                      batch_size = batch_size, # batch size\n","                                      shuffle =True, # Whether to shuffle your data for every epoch. (Very important for training performance)\n","                                      num_workers = 1) # number of workers to load your data. (usually number of cpu cores)\n","\n","    testDataLoader = data.DataLoader(dataset = mnist_test, \n","                                    batch_size = batch_size,\n","                                    shuffle = False, # we don't actually need to shuffle data for test\n","                                    num_workers = 1) #\n","    print (\"[+] Finished loading data \u0026 Preprocessing\")\n","    return mnist_train,mnist_test,trainDataLoader,testDataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9Gr4wczGeLz","outputId":"60456c65-e57c-486b-9b6d-c7502983f74e","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[+] Get the MNIST DATA\n","[+] Finished loading data \u0026 Preprocessing\n"]}],"source":["trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader "]},{"cell_type":"markdown","metadata":{"id":"Tbb7EJxWGeL0"},"source":["## Define Trainer\n","\n","Trainer를 class로 만든 것입니다. 목적은 net, trainloader, testloader, optimizer, criterion을 모두 input으로 받아, Trainer 내에서 모델의 학습과 평가를 하기 위함입니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxsjtcDzGeL0"},"outputs":[],"source":["class Trainer():\n","    def __init__(self, trainloader, testloader, net, optimizer, criterion):\n","        \"\"\"\n","        trainloader: train data's loader\n","        testloader: test data's loader\n","        net: model to train\n","        optimizer: optimizer to update your model\n","        criterion: loss function\n","        \"\"\"\n","        self.trainloader = trainloader\n","        self.testloader = testloader\n","        self.net = net\n","        self.optimizer = optimizer\n","        self.criterion = criterion\n","        \n","    def train(self, epoch = 1):\n","        \"\"\"\n","        epoch: number of times each training sample is used\n","        \"\"\"\n","        self.net.train()\n","        for e in range(epoch):\n","            running_loss = 0.0  \n","            for i, data in enumerate(self.trainloader, 0): \n","                # get the inputs\n","                inputs, labels = data # Return type for data in dataloader is tuple of (input_data, labels)\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","                # zero the parameter gradients\n","                self.optimizer.zero_grad()    \n","\n","                # forward + backward + optimize\n","                outputs = self.net(inputs) # get output after passing through the network\n","                loss = self.criterion(outputs, labels) # compute model's score using the loss function \n","                loss.backward() # perform back-propagation from the loss\n","                self.optimizer.step() # perform gradient descent with given optimizer\n","\n","                # print statistics\n","                running_loss += loss.item()\n","                if (i+1) % 500 == 0:    # print every 2000 mini-batches\n","                    print('[%d, %5d] loss: %.3f' % (e + 1, i + 1, running_loss / 500))\n","                    running_loss = 0.0\n","\n","        print('Finished Training')\n","        \n","    def test(self):\n","        self.net.eval() \n","        \n","        test_loss = 0\n","        correct = 0\n","        for inputs, labels in self.testloader:\n","            inputs = inputs.cuda()\n","            labels = labels.cuda() \n","            output = self.net(inputs) \n","            pred = output.max(1, keepdim=True)[1] # get the index of the max \n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","\n","            test_loss /= len(self.testloader.dataset)\n","        print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.\n","                format(correct, len(self.testloader.dataset),\n","                100.* correct / len(self.testloader.dataset)))\n","        \n","    def get_conf(self):\n","        self.net.eval()\n","        \n","        confusion = torch.zeros(10,10)\n","        for inputs, labels in self.testloader:\n","            inputs = inputs.cuda()\n","            labels = labels.cuda()\n","            output = self.net(inputs)\n","            pred = torch.argmax(output, dim=1)\n","            \n","            for num in range(output.shape[0]):\n","                confusion[pred[num], labels[num]] += 1\n","        \n","        return confusion"]},{"cell_type":"markdown","metadata":{"id":"fXxrBYMFGeL1"},"source":["## Let's create Model by yourself"]},{"cell_type":"markdown","metadata":{"id":"MH6OtFVNGeL5"},"source":["## Batch-Normalization"]},{"cell_type":"markdown","metadata":{"id":"QpfH6WKOGeL5"},"source":["### (1) 2-Layer Network + ReLU + Adam + Batch-Norm\n","\n","- Input: (28 * 28)\n","- Hidden dimension: (30)\n","- Output dimension: 10\n","- activation: relu\n","- normalization: batch-norm\n","- Optimizer: Adam\n","- Loss: Cross-Entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUCdy1inGeL5"},"outputs":[],"source":["class MNIST_Net(nn.Module):\n","    def __init__(self):\n","        super(MNIST_Net, self).__init__() \n","        # an affine operation: y = Wx + b\n","        self.fc0 = nn.Linear(28*28,30)\n","        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n","        self.fc1 = nn.Linear(30, 10)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = x.view(-1,28*28) \n","        x = self.fc0(x) # 28*28 -\u003e 30 \n","        x = self.bn0(x)\n","        x = self.act(x) \n","        x = self.fc1(x)   \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"humUDIcAGeL6"},"outputs":[],"source":["mnist_net = MNIST_Net().cuda() \n","criterion = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_E03DeSuGeL6"},"outputs":[],"source":["trainer = Trainer(trainloader = trainDataLoader,\n","                  testloader = testDataLoader,\n","                  net = mnist_net,\n","                  criterion = criterion,\n","                  optimizer = optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqNR1EsAGeL6","outputId":"e573709a-6895-40a2-b833-cb6853bffd02"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,   500] loss: 0.716\n","[1,  1000] loss: 0.340\n","[1,  1500] loss: 0.275\n","[2,   500] loss: 0.217\n","[2,  1000] loss: 0.217\n","[2,  1500] loss: 0.203\n","[3,   500] loss: 0.174\n","[3,  1000] loss: 0.164\n","[3,  1500] loss: 0.177\n","[4,   500] loss: 0.142\n","[4,  1000] loss: 0.152\n","[4,  1500] loss: 0.144\n","Finished Training\n"]}],"source":["trainer.train(epoch = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVgHcY1ZGeL6","outputId":"e4c137f5-e86b-48a9-c4fc-a63dd4afd541"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set:  Accuracy: 9639/10000 (96%)\n","\n"]}],"source":["trainer.test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzOppTIuGeL6","outputId":"c186c1ab-251b-4ec9-928e-453a8e928496"},"outputs":[{"data":{"text/plain":["23920"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","count_parameters(mnist_net)"]},{"cell_type":"markdown","metadata":{"id":"RFOlwireGeL6"},"source":["### (2) 3-Layer Network (Conv+Fc) + ReLU + Adam + Batch-Norm\n","\n","- Input: (28 * 28)\n","- Conv: 8 (6 * 6) filter with stride=2 \n","- Hidden dimension: 8 * 12 * 12\n","- Output dimension: 10\n","- activation: relu\n","- normalization: batch-norm\n","- Optimizer: Adam\n","- Loss: Cross-Entropy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wC3XDkV7GeL7"},"outputs":[],"source":["class MNIST_Net(nn.Module):\n","    def __init__(self):\n","        super(MNIST_Net, self).__init__()\n","        self.conv0 = nn.Conv2d(in_channels = 1,\n","                               out_channels = 8,\n","                               kernel_size = 6,\n","                               stride = 2) # Layer 1\n","        self.conv0_bn = nn.BatchNorm2d(8)  # 2d batch-norm is used in 3d inputs\n","        self.fc = nn.Linear(8*12*12, 10)   # Layer 2 \n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv0(x)\n","        x = self.conv0_bn(x)\n","        x = self.act(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBSQSoQqGeL7"},"outputs":[],"source":["mnist_net = MNIST_Net().cuda() \n","criterion = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6oKQ-lhGeL7"},"outputs":[],"source":["trainer = Trainer(trainloader = trainDataLoader,\n","                  testloader = testDataLoader,\n","                  net = mnist_net,\n","                  criterion = criterion,\n","                  optimizer = optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvbkzLw2GeL7","outputId":"2ee3bbd3-576e-4e85-c5ac-243c990a4dab"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,   500] loss: 0.399\n","[1,  1000] loss: 0.176\n","[1,  1500] loss: 0.135\n","[2,   500] loss: 0.088\n","[2,  1000] loss: 0.090\n","[2,  1500] loss: 0.077\n","[3,   500] loss: 0.056\n","[3,  1000] loss: 0.065\n","[3,  1500] loss: 0.063\n","[4,   500] loss: 0.054\n","[4,  1000] loss: 0.053\n","[4,  1500] loss: 0.048\n","Finished Training\n"]}],"source":["trainer.train(epoch = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGlkiz_PGeL7","outputId":"40973cac-4950-43b9-b96f-b87c92c0562c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set:  Accuracy: 9820/10000 (98%)\n","\n"]}],"source":["trainer.test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CcXA4voZGeL7","outputId":"6bf4bc1e-1bfc-4919-8fba-228099d69ad4"},"outputs":[{"data":{"text/plain":["11842"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(mnist_net)"]},{"cell_type":"markdown","metadata":{"id":"t45KeuyNGeL7"},"source":["### (3) 3-Layer Network (Conv+Pool+Fc) + ReLU + Adam + Batch-Norm\n","\n","- Input: (28 * 28)\n","- Conv: 8 (7 * 7) filter with stride=2 \n","- Pool: 2 * 2\n","- Hidden dimension: 8 * 6 * 6\n","- Output dimension: 10\n","- activation: relu\n","- normalization: batch-norm\n","- Optimizer: Adam\n","- Loss: Cross-Entropy"]},{"cell_type":"markdown","metadata":{"id":"UsNZoI3gGeL7"},"source":["### Pooling Operation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wABH6T9bGeL7"},"outputs":[],"source":["class MNIST_Net(nn.Module):\n","    def __init__(self):\n","        super(MNIST_Net, self).__init__()\n","        self.conv0 = nn.Conv2d(in_channels = 1,\n","                               out_channels = 8,\n","                               kernel_size = 6,\n","                               stride = 2) # Layer 1\n","        self.conv0_bn = nn.BatchNorm2d(8)  \n","        self.pool0 = nn.MaxPool2d(2)\n","        self.fc = nn.Linear(8*6*6, 10) \n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.conv0(x)\n","        x = self.conv0_bn(x)\n","        x = self.act(x)\n","        x = self.pool0(x)\n","        x = x.view(x.shape[0], -1)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfe0l5eKGeL7"},"outputs":[],"source":["mnist_net = MNIST_Net().cuda() \n","criterion = nn.CrossEntropyLoss()  \n","optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omjUfIuoGeL7"},"outputs":[],"source":["trainer = Trainer(trainloader = trainDataLoader,\n","                  testloader = testDataLoader,\n","                  net = mnist_net,\n","                  criterion = criterion,\n","                  optimizer = optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHXy2JqjGeL7","outputId":"e076f0c8-320d-4c25-a353-cc2520d6c2aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,   500] loss: 0.560\n","[1,  1000] loss: 0.199\n","[1,  1500] loss: 0.149\n","[2,   500] loss: 0.111\n","[2,  1000] loss: 0.100\n","[2,  1500] loss: 0.095\n","[3,   500] loss: 0.083\n","[3,  1000] loss: 0.079\n","[3,  1500] loss: 0.076\n","[4,   500] loss: 0.065\n","[4,  1000] loss: 0.070\n","[4,  1500] loss: 0.069\n","Finished Training\n"]}],"source":["trainer.train(epoch = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ze3-uFZ6GeL7","outputId":"23fe12a0-e96f-4153-c05d-054a4543e117"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Test set:  Accuracy: 9765/10000 (98%)\n","\n"]}],"source":["trainer.test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20jQxTA_GeL8","outputId":"538e76d9-3a2c-4f94-c310-7572599e1e4b"},"outputs":[{"data":{"text/plain":["3202"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["count_parameters(mnist_net)"]}],"metadata":{"colab":{"name":"실습3. 합성곱 MNIST.ipynb","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}