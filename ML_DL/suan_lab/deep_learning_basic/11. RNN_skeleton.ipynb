{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_11 RNN.ipynb의 사본","provenance":[{"file_id":"1OEOdA2JEH2V9zrgqFYywvwxUxdcZGyg5","timestamp":1659606142846}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eT8Pk_xTF75M"},"source":["# 자연어 처리(Natural Language Processing, NLP)\n","\n","- 한국어, 영어 등 우리가 평소에 쓰는 언어\n","\n","- 사람의 말을 컴퓨터가 이해하도록 수행하는 과정"]},{"cell_type":"markdown","metadata":{"id":"XoWecPwRF8sw"},"source":["## 신경망에서의 단어 처리 (단어 임베딩, Word Embedding)\n","- 단어를 있는 그대로 처리하지 않고 고정 길이의 벡터로 표현 (원-핫 인코딩)\n","\n","- 단어 하나에 인덱스 정수를 할당하는 Bag of Words 방법\n","\n","- 예시)  \n","  \"you\", \"are\", \"not\", \"a\", \"smart\", \"student\"\n","\n","  - \"you\"     : 0\n","  - \"are\"     : 1\n","  - \"not\"     : 2\n","  - \"a\"       : 3\n","  - \"smart\"   : 4\n","  - \"student\" : 5\n","\n","        \"You are a smart student.\"  \n","    ---> [1, 1, 0, 1, 1, 1]\n","\n","  <br>\n","  \n","  <img src=\"https://miro.medium.com/max/1348/1*YEJf9BQQh0ma1ECs6x_7yQ.png\" width=\"600\">\n","\n","  <sub>출처: https://medium.com/@athif.shaffy/one-hot-encoding-of-text-b69124bef0a7</sub>"]},{"cell_type":"markdown","metadata":{"id":"wga06Nz-F-qI"},"source":["## 신경망에서의 단어처리 구조\n","\n","<img src=\"https://miro.medium.com/max/1000/1*1O5gLhOg25HviI8bwZxV4g.png\" width=\"600\">\n","\n","<sub>출처: https://mc.ai/deep-nlp-word-vectors-with-word2vec/</sub>"]},{"cell_type":"markdown","metadata":{"id":"g4Yk95bFTU27"},"source":["## CBOW (Continuous Bag of Words) Embedding\n","- 복수 단어 문맥(multi-word context)에 대한 문제  \n","  즉, 여러개의 단어를 나열한 뒤 이와 관련된 단어를 추정하는 문제\n","\n","- 예를 들어,  \n","      Betty bought a bit of better butter.\n","\n","  위 예시에서 (Betty, a bit, butter)라는 문맥이 주어지면 bought를 예측하는 구조\n","\n","  <br>\n","\n","  <img src=\"https://miro.medium.com/max/604/1*DfuBd49nCtT99h328iXL2Q.png\" width=\"300\">\n","\n","  <sub>출처: https://medium.com/@srishtee.kriti/mathematics-behind-continuous-bag-of-words-cbow-model-1e54cc2ecd88</sub>"]},{"cell_type":"markdown","metadata":{"id":"bxa8iUpaTVsB"},"source":["## Skip-Gram Embedding\n","- CBOW 방식과 반대\n","\n","- 특정한 단어로부터 문맥이 될 수 있는 단어를 예측\n","\n","- 보통 입력 단어 주변의  $k$ 개 단어를 문맥으로 보고 예측 모형을 만드는데 이  $k$ 값을 window size 라고 한다.\n","\n","- 예시) window size = 2 라면,  \n","      Betty -> bought, butter  \n","      bought -> butter, Betty  \n","      a -> bit, of  \n","\n","  <br>\n","\n","  <img src=\"https://www.researchgate.net/publication/322905432/figure/fig1/AS:614314310373461@1523475353979/The-architecture-of-Skip-gram-model-20.png\" width=\"300\">\n","\n","  <sub>출처: https://www.researchgate.net/figure/The-architecture-of-Skip-gram-model-20_fig1_322905432</sub>"]},{"cell_type":"markdown","metadata":{"id":"ZgQhTm1BTYt1"},"source":["## Word2Vec\n","- CBOW, Skip_Gram 방식의 단어 임베딩을 구현.  \n","  구글의 Mikolov 개발\n","\n","- subsampling, negative sampling 등의 기법 추가하여 학습 속도 향상\n","\n","  <img src=\"https://mbenhaddou.com/wp-content/uploads/2019/12/img_4.png\">\n","\n","  <sub>출처: http://mbenhaddou.com/2019/12/14/word2vec-concepts-from-scratch/</sub>\n","\n","  <br>\n","\n","  <img src=\"https://miro.medium.com/max/2456/1*gcC7b_v7OKWutYN1NAHyMQ.png\" width=\"600\">\n","\n","  <sub>출처: https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4</sub>"]},{"cell_type":"markdown","metadata":{"id":"26OlAvwKTbYz"},"source":["## Word2Vec 예제\n","\n","- 출처: https://datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/"]},{"cell_type":"code","metadata":{"id":"1ABlKJAcTcw4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrIAzCzwTc_l"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PB7kBcQETdPv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YO2hFz3iTda7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXJvPZZyTdV9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hqQq37bITdIw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hNQ_O18TdGs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L95sLAsbTc4R"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-U_PxqMTc20"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpIdkITrTwHu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5PYKGZgTwSB"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFyGsWqpT4tP"},"source":["### 네이버 영화 감상 코퍼스를 사용한 한국어 단어 임베딩\n","- 한국어 임베딩은 \"konlpy\" 필요"]},{"cell_type":"code","metadata":{"id":"K5awuhIsTwUz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV16Az5MeOxr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbIDwP6iTwZP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C60vas6reXb3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU9ab9fhTwhi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtGSMyX4Twf1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUvMyPKUTwMo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aMscjdX9Tcr1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXa0zrC5UNZT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_4_3j4YdUNQz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jacLnynuqUXZ"},"source":["# 순환 신경망 (Recurrent Neural Network, RNN)\n","\n","- **순서가 있는 데이터**를 입력으로 받음\n","\n","- 변화하는 입력에 대한 출력을 얻음\n","\n","- 시계열(날씨, 주가 등), 자연어와 같이 **시간의 흐름에 따라 변화하고, 그 변화가 의미를 갖는 데이터** "]},{"cell_type":"markdown","metadata":{"id":"3P0gpo4gqWUz"},"source":["## Feed Forward Network vs Recurrent Network\n","\n","- Feed Forward Net (앞먹임 구조)\n","  - 일반적인 구조의 신경망\n","\n","  - 입력 → 은닉 → 출력층 으로 이어지는 단방향 구조\n","\n","  - 이전 스텝의 출력의 영향을 받지 않음\n","\n","- Recurrent Net (되먹임 구조)\n","  - 이전 층(Layer), 또는 스텝의 출력이 다시 입력으로 연결되는 신경망 구조\n","\n","  - 각 스텝마다 이전 상태를 기억 시스템(Memory System)  \n","\n","  - 현재 상태가 이전 상태에 종속\n","\n","  <br>\n","\n","  <img src=\"https://www.researchgate.net/profile/Engin_Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png\">\n","\n","  <sub>출처: https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480</sub>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2h5HFH0BqYho"},"source":["## 순환 신경망 구조\n","\n","<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"700\">\n","\n","<br>\n","\n","- 입력 $x_t$에서 $t$는 시각을 뜻함\n","\n","- $X_0$에 대한 출력 $Y_0$이 다음 레이어에 전달\n","\n","- 각각의 입력에 대해 출력은 해당 레이어대로 출력값을 반환"]},{"cell_type":"markdown","metadata":{"id":"kowOGfSLqbSn"},"source":["## 순환 신경망의 다양한 구조\n","\n","<img src=\"https://static.packt-cdn.com/products/9781789346640/graphics/2d4a64ef-9cf9-4b4a-9049-cb9de7a07f89.png\">\n","  \n","  <sub>출처: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789346640/11/ch11lvl1sec80/introduction</sub>\n","\n","- one to one\n","  - RNN\n","\n","- one to many\n","  - Image Captioning \n","\n","  - 이미지에 대한 설명 생성\n","\n","- many to one\n","  - Sentiment Classification\n","\n","  - 문장의 긍정/부정을 판단하는 감정 분석\n","\n","- many to many\n","  - Machine Translation\n","\n","  - 하나의 언어를 다른 언어로 번역하는 기계 번역\n","\n","- many to many\n","  - Video Classification(Frame Level)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uiIA4Ue7-G5B"},"source":["## 두 가지 정보(현재 입력, 이전 시각의 출력)을 처리하는 수식\n","$\\qquad h_t = tanh ( \\ h_{t-1} W_h \\ + \\ x_t W_x + b) $\n","\n","- $W_x$ : 입력 $x$를 출력 $h$로 변환하기 위한 가중치\n","\n","- $W_h$ : 다음 시각의 출력으로 변환하기 위한 가중치\n","\n","- $h$는 '상태'를 기억\n","\n","- $h_t \\ $를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라고도 불림\n","\n","  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"]},{"cell_type":"markdown","metadata":{"id":"mNUu9LzO-KcF"},"source":["## BPTT(BackPropagation Through Time)\n","\n","- 시간 방향으로 펼친 신경망의 오차역전파\n","\n","- 시계열 데이터의 시간 크기가 커지면 역전파 시 불안정해짐\n","\n","- 기울기 소실 문제 발생\n","\n","  <img src=\"https://iamtrask.github.io/img/backprop_through_time.gif\" width=\"700\">\n","\n","  <sub>출처: https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</sub>"]},{"cell_type":"markdown","metadata":{"id":"HIJmsDgw-WWS"},"source":["## Truncated BPTT\n","\n","- 큰 시계열 데이터를 다룰 때 사용하는 오차역전파법\n","\n","- 신경망을 **적당한 길이로 끊는다.**\n","  - <u>역전파에 연결만! 순전파의 연결은 끊어지지 않는다.</u>\n","\n","- 학습 시, 입력이 **순서대로 연결**되어 입력해야 함\n","\n","  <img src=\"https://r2rt.com/static/images/RNN_true_truncated_backprop.png\">\n","\n","  <sub>출처: https://r2rt.com/styles-of-truncated-backpropagation.html</sub>\n"]},{"cell_type":"markdown","metadata":{"id":"9dpRw1XV-Y9n"},"source":["## RNN 구현\n","\n","- 형상 주의!\n","\n","## $\\qquad \\ h_{t-1} W_h \\ + \\ x_t W_x = h_t$\n","\n","- $h_{t-1}$ : $N \\times H$\n","\n","- $W_{h}$ : $H \\times H$\n","\n","- $x_{t}$ : $N \\times D$\n","\n","- $W_{x}$ : $D \\times H$\n","\n","- $h_t$ : $N \\times H$\n","\n","- $D$ : 입력 벡터의 차원 수\n","\n","- $H$ : 은닉 상태 벡터의 차원 수\n"]},{"cell_type":"code","metadata":{"id":"Arjp1LQbuVc5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRZ47KbtFj8O"},"source":["## Time RNN Layer\n","- RNN 계층의 은닉상태 $h$를 가지고 있음\n"," "]},{"cell_type":"code","metadata":{"id":"VQhOF1RHFhVH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzFqvo2aFpMb"},"source":["# LSTM (Long Shot-Term Memory)\n","\n","- RNN은 장기 기억을 제대로 처리 못함\n","\n","- RNN은 기울기 소실 또는 기울기 폭발을 발생\n","\n","- 위를 해결하기 위해 LSTM 방법 등장\n","\n","  <img src=\"https://www.researchgate.net/publication/324600237/figure/fig3/AS:616974623178753@1524109621725/Long-Short-term-Memory-Neural-Network.png\" width=\"600\">\n","\n","<sub>출처: https://www.researchgate.net/figure/Long-Short-term-Memory-Neural-Network_fig3_324600237</sub>\n","\n","### $\\qquad f = \\sigma (x_t W^{(f)}_x + h_{t-1} W^{(f)}_h + b^{(f)} \\\\ \n","\\qquad g = tanh(x_t W^{(g)}_x + h_{t-1} W^{(g)}_h + b^{(g)}) \\\\\n","\\qquad i = \\sigma(x_t W^{(i)}_x + h_{t-1} W^{(i)}_h + b^{(i)}) \\\\\n","\\qquad o = tanh(x_t W^{(o)}_x + h_{t-1} W^{(o)}_h + b^{(o)})$\n","\n","\n","### $\\qquad c_t = f \\odot c_{t-1} + g \\odot i \\\\\n","\\qquad h_t = o \\odot tanh(c_t)\n","$\n"]},{"cell_type":"markdown","metadata":{"id":"ii53247RFu9N"},"source":["## forget gate (망각 게이트)\n","- 불필요한 정보를 잊는 게이트\n","\n","-  $h_{t−1}$ 과 $x_t$ 를 받아 시그모이드를 취해준 값이 forget gate의 출력값\n","\n","- 시그모이드 함수를 통과하기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전 상태의 정보를 온전히 기억\n","\n","  <img src=\"https://image.slidesharecdn.com/dlsl2017d2l2recurrentneuralnetworksi-170125171004/95/recurrent-neural-networks-i-d2l2-deep-learning-for-speech-and-language-upc-2017-25-638.jpg?cb=1485365064\" width=\"600\">\n","\n","  <sub>출처: https://www.slideshare.net/xavigiro/recurrent-neural-networks-1-d2l2-deep-learning-for-speech-and-language-upc-2017</sub>"]},{"cell_type":"markdown","metadata":{"id":"XXhr1CGwF1S2"},"source":["## input gate (입력 게이트)\n","- 현재 정보를 기억하기’ 위한 게이트\n","\n","- $h_{t−1}$ 과 $x_t$를 받아 $Sigmoid$ -> $tanh$ 를 통과한 다음,  \n","  Hadamard product 연산을 한 값을 출력\n","  \n","  <br>\n","\n","  <img src=\"https://image.slidesharecdn.com/dlmmdcud2l08recurrentneuralnetworks-170429103823/95/recurrent-neural-networks-d2l8-insightdcu-machine-learning-workshop-2017-28-638.jpg?cb=1493462658\" width=\"600\">\n","\n","  <sub>출처: https://www.slideshare.net/xavigiro/recurrent-neural-networks-1-d2l2-deep-learning-for-speech-and-language-upc-2017</sub>"]},{"cell_type":"markdown","metadata":{"id":"SyrTIpZhFxyR"},"source":["## output gate (출력 게이트)\n","\n","- 은닉 상태 $h_t$의 출력을 담당하는 게이트\n","\n","- 입력 $x_t$와 이전 상태 $h_{t-1}$로부터 게이트의 열림 상태가 결정됨\n","\n","  <img src=\"https://image.slidesharecdn.com/dlcvd2l6recurrentneuralnetworks-160802094750/95/deep-learning-for-computer-vision-recurrent-neural-networks-upc-2016-30-638.jpg?cb=1470131837\" width=\"600\">\n","\n","<sub>출처: https://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-recurrent-neural-networks-upc-2016</sub>"]},{"cell_type":"markdown","metadata":{"id":"LJVkn0D6F3Yz"},"source":["## LSTM 구현"]},{"cell_type":"code","metadata":{"id":"gjrZLUk8Fpba"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dc0wuvArGHgD"},"source":["# GRU (Gated Recurrent Unit)\n","- LSTM을 더 단순하게 만든 구조\n","\n","- 기억 셀은 없고, 시간방향으로 전파하는 것은 은닉 상태만 있음\n","\n","- reset gate\n","  - 과거의 은닉 상태를 얼마나 무시할지 결정\n","\n","  - $r$ 값이 결정\n","\n","- update gate\n","  -  은닉 상태를 갱신하는 게이트  \n","\n","  - LSTM의 forget, input gate 역할을 동시에 함\n","  \n","  <img src=\"https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png\" width=\"500\">\n","\n","<sub>출처: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub>\n","\n","  ### $\\qquad z = \\sigma (x_t W^{(z)}_x + h_{t-1} W^{(z)}_h + b^{(z)} \\\\ \n","  \\qquad r = \\sigma (x_t W^{(r)}_x + h_{t-1} W^{(r)}_h + b^{(r)}) \\\\\n","  \\qquad \\tilde{i} = tanh (x_t W^{(i)}_x + (r \\odot h_{t-1}) W^{(i)}_h + b ) \\\\\n","  \\qquad h_t = (1 - z) \\odot h_{t-1} + z \\odot \\tilde{h}$\n"]},{"cell_type":"markdown","metadata":{"id":"n96GGOgsGKhE"},"source":["# (참고) RNN vs LSTM vs GRU\n","\n","<img src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1849/http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU.png\">\n","\n","<sub>출처: http://dprogrammer.org/rnn-lstm-gru</sub>"]}]}