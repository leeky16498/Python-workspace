{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7. 신경망 학습_answers","provenance":[{"file_id":"1hk4I8R9TidWzFQrKTJjuij54kN_QC3jY","timestamp":1659440009132}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"30a71f72fcd5467385ad9f5f0ca0c3cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5d660bcaec642c7bda85a5993e3472d","IPY_MODEL_14739c2e4170434aac558714163e00b9","IPY_MODEL_3585c82c62d64330bcaa37477d19f14c"],"layout":"IPY_MODEL_f530039c91de4f06a511695e75c3d2bb"}},"a5d660bcaec642c7bda85a5993e3472d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0596f0d416624eed801c4b254ab6f98e","placeholder":"​","style":"IPY_MODEL_d0d169de9e8b4fcc881c95c4f2673462","value":"100%"}},"14739c2e4170434aac558714163e00b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a2366e9a836460192cd6660ab8da678","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7232cc20147436b8598993e6e43bfad","value":2}},"3585c82c62d64330bcaa37477d19f14c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14b20fab96634f61b7d874383e692793","placeholder":"​","style":"IPY_MODEL_029b34ed83894605a093c2333f46cff2","value":" 2/2 [03:10&lt;00:00, 95.21s/it]"}},"f530039c91de4f06a511695e75c3d2bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0596f0d416624eed801c4b254ab6f98e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0d169de9e8b4fcc881c95c4f2673462":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2366e9a836460192cd6660ab8da678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7232cc20147436b8598993e6e43bfad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"14b20fab96634f61b7d874383e692793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"029b34ed83894605a093c2333f46cff2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"nyjyu4FzUAVw"},"source":["# 신경망 학습"]},{"cell_type":"markdown","metadata":{"id":"VQvNez4qydhL"},"source":["## 단순한 신경망 구현 : Logic Gate"]},{"cell_type":"markdown","metadata":{"id":"-7te43hqyiiJ"},"source":["### 필요한 모듈 import"]},{"cell_type":"code","metadata":{"id":"Qf2F_YbdybBE"},"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","plt.style.use(\"seaborn\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orUoPmDcymhj"},"source":["### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","metadata":{"id":"bOAmMxo0ymDF"},"source":["# 하이퍼 파라미터는 사용자가 직접 통제하는 상수로, 학습 전 지정된다.\n","epochs = 1000 \n","lr = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjmLWgFVysnq"},"source":["### 유틸 함수들(Util Functions)"]},{"cell_type":"code","metadata":{"id":"Y4OMFGrjyq1c"},"source":["# 대표적인 활성화 함수 리스트\n","\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x))\n","\n","def mean_squared_error(pred_y, true_y):\n","    return 0.5 * (np.sum((true_y - pred_y)**2))\n","\n","def cross_entropy_error(pred_y, true_y):\n","    if true_y.ndim == 1:\n","        true_y = true_y.reshape(1, -1)\n","        pred_y = pred_y.reshape(1, -1)\n","\n","    delta = 1e-7\n","    return -np.sum(true_y * np.log(pred_y + delta))\n","\n","def cross_entropy_error_for_batch(pred_y, true_y):\n","    if true_y.ndim == 1:\n","        true_y = true_y.reshape(1, -1)\n","        pred_y = pred_y.reshape(1, -1)\n","\n","    delta = 1e-7\n","    batch_size = pred_y.shape[0]\n","\n","    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n","\n","def cross_entropy_for_bin(pred_y, true_y):\n","    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1-pred_y)))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","\n","    return y\n","\n","def differential(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","        temp_val = x[i]\n","\n","        x[i] = temp_val + eps\n","        f_h1 = f(x)\n","\n","        x[i] = temp_val - eps\n","        f_h2 = f(x)\n","\n","        diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","        x[i] = temp_val\n","\n","    return diff_value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5Z2LTT_y3i5"},"source":["### 신경망"]},{"cell_type":"code","metadata":{"id":"gMTjjYgdy3D8","executionInfo":{"status":"ok","timestamp":1659877829396,"user_tz":-60,"elapsed":5,"user":{"displayName":"이경윤","userId":"09327853121914493654"}}},"source":["class LogicGateNet():\n","\n","    def __init__(self):\n","        def weight_init():\n","            np.random.seed(1)\n","            weights = np.random.randn(2)\n","            bias = np.random.rand(1)\n","\n","            return weights, bias\n","        \n","        self.weights, self.bias = weight_init()\n","\n","    def predict(self, x):\n","        W = self.weights.reshape(-1, 1)\n","        b = self.bias\n","\n","        pred_y = sigmoid(np.dot(x, W) + b)\n","        return pred_y\n","\n","    def loss(self, x, true_y):\n","        pred_y = self.predict(x)\n","        return cross_entropy_for_bin(pred_y, true_y)\n","\n","    def get_gradient(self, x, t):\n","        def loss_gradient(grad):\n","            return self.loss(x, t)\n","        \n","        grad_W = differential(loss_gradient, self.weights)\n","        grad_b = differential(loss_gradient, self.bias)\n","\n","        return grad_W, grad_b"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wbNDoH_3zbGZ"},"source":["\n","### AND Gate"]},{"cell_type":"markdown","metadata":{"id":"2P-ib8_RzHTh"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"rRiaACA6zGom"},"source":["AND = LogicGateNet()\n","\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y = np.array([[0], [0], [0], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","    grad_W, grad_B = AND.get_gradient(X, Y)\n","\n","    AND.weights -= lr * grad_W\n","    AND.bias -= lr * grad_B\n","\n","    loss = AND.loss(X, Y)\n","\n","    train_loss_list.append(loss)\n","\n","    if i%100 == 99:\n","        print(\"epoch : {}, cost : {}, weight : {}, bias : {}\".format(i+1, loss, AND.weights, AND.bias))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZoyQv_czT7R"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"-7CvWgc9zREa"},"source":["print(AND.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HoMXNiXWzts-"},"source":["### OR Gate"]},{"cell_type":"markdown","metadata":{"id":"DZ79pc4jzw3O"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"8gnLmAyQzuoL"},"source":["OR = LogicGateNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_2 = np.array([[0], [0], [0], [1]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","    grad_W, grad_B = OR.get_gradient(X, Y_2)\n","\n","    OR.weights -= lr * grad_W\n","    OR.bias -= lr * grad_B\n","\n","    loss = OR.loss(X, Y_2)\n","    train_loss_list.append(loss)\n","\n","    if i%100 == 99:\n","        print(\"epoch : {}, cost : {}, weight : {}, bias : {}\".format(i+1, loss, OR.weights, OR.bias))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWmEtX_VnLSI"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"JwPpOs3-z2vU"},"source":["print(OR.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEBhczCIz57Q"},"source":["### NAND Gate"]},{"cell_type":"markdown","metadata":{"id":"TzQaaHKKz8sZ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"h463QUQRz8PS"},"source":["NAND = LogicGateNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_3 = np.array([[1], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","    grad_W, grad_B = NAND.get_gradient(X, Y_3)\n","\n","    NAND.weights -= lr * grad_W\n","    NAND.bias -= lr * grad_B\n","\n","    loss = OR.loss(X, Y_3)\n","    train_loss_list.append(loss)\n","    \n","    if i%100 == 99:\n","        print(\"epoch : {}, cost : {}, weight : {}, bias : {}\".format(i+1, loss, NAND.weights, NAND.bias))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jR-rHaTU0Mga"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"WpzKW6sm0Ghp"},"source":["print(NAND.predict(X))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiTWfSQ60Zl2"},"source":["### XOR Gate"]},{"cell_type":"markdown","metadata":{"id":"hmmL0VIu0bXq"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"0CGm0r1M0a9M"},"source":["XOR = LogicGateNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_4 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","    grad_W, grad_B = XOR.get_gradient(X, Y_4)\n","\n","    XOR.weights -= lr * grad_W\n","    XOR.bias -= lr * grad_B\n","\n","    loss = XOR.loss(X, Y_3)\n","    train_loss_list.append(loss)\n","    \n","    if i%100 == 99:\n","        print(\"epoch : {}, cost : {}, weight : {}, bias : {}\".format(i+1, loss, XOR.weights, XOR.bias))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cy-ktElI0o5P"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"GWAJAJ_T0oqm"},"source":["print(XOR.predict(X))\n","# 전부 0.5단위로 학습이 잘 안된다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAlq_-6E1nIq"},"source":["#### 2층 신경망으로 XOR 게이트 구현(1)\n","\n","- 얕은 신경망, Shallow Neural Network\n","\n","- 두 논리게이트(NAND, OR)를 통과하고  \n","  AND 게이트로 합쳐서 구현\n","\n","- 06 신경망 구조 참고"]},{"cell_type":"code","metadata":{"id":"mr7nYMG20jTo"},"source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","s1 = NAND.predict(X)\n","s2 = OR.predict(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkTDx8Ah1xHY"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"LK2iD5A91yWQ"},"source":["X_2 = np.array([s1, s2]).T.reshape(-1, 2)\n","print(AND.predict(X_2))\n","# 테스트 다시 해보기, 0, 1, 1, 0으로 잘 나와야 한다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-SK4G262Agn"},"source":["#### 2층 신경망으로 XOR 게이트 구현(2)\n","- 클래스로 구현"]},{"cell_type":"code","metadata":{"id":"8RpnHCRZ1zwr"},"source":["class XORNet():\n","    def __init__(self):\n","        np.random.seed(1)\n","\n","        def weight_init():\n","            params = {}\n","            params[\"w_1\"] = np.random.randn(2)\n","            params[\"b_1\"] = np.random.rand(2)\n","            params[\"w_2\"] = np.random.randn(2)\n","            params[\"b_2\"] = np.random.rand(1)\n","            return params\n","\n","        self.params = weight_init()\n","\n","    def predict(self, x):\n","        W_1, W_2 = self.params[\"w_1\"].reshape(-1, 1), self.params[\"w_2\"].reshape(-1, 1)\n","        B_1, B_2 = self.params[\"b_1\"], self.params[\"b_2\"]\n","\n","        A1 = np.dot(x, W_1) + B_1 \n","        Z1 = sigmoid(A1)\n","        A2 = np.dot(Z1, W_2) + B_2\n","        pred_y = sigmoid(A2)\n","\n","        return pred_y\n","\n","    def loss(self, x, true_y):\n","        pred_y = self.predict(x)\n","        return cross_entropy_for_bin(pred_y, true_y)\n","\n","    def get_gradient(self, x, t):\n","        def loss_grad(grad):\n","            return self.loss(x, t)\n","        grads = {}\n","        grads[\"w_1\"] = differential(loss_grad, self.params[\"w_1\"])\n","        grads[\"b_1\"] = differential(loss_grad, self.params[\"b_1\"])\n","        grads[\"w_2\"] = differential(loss_grad, self.params[\"w_2\"])\n","        grads[\"b_2\"] = differential(loss_grad, self.params[\"b_2\"])\n","\n","        return grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lplK_x0l2YLh"},"source":["#### 하이퍼 파라미터(Hyper Parameter)\n","- 재조정"]},{"cell_type":"code","metadata":{"id":"qf-3wWSv2b7l"},"source":["lr = 0.3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmHKd45d2JbJ"},"source":["#### 모델 생성 및 학습"]},{"cell_type":"code","metadata":{"id":"cQNd3XVd2Gj7"},"source":["XOR = XORNet()\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","Y_5 = np.array([[0], [1], [1], [0]])\n","\n","train_loss_list = list()\n","\n","for i in range(epochs):\n","    grads = XOR.get_gradient(X, Y_5)\n","\n","    for key in ('w_1', \"b_1\", \"w_2\", \"b_2\"):\n","        XOR.params[key] -= lr * grads[key]\n","\n","    loss = XOR.loss(X, Y_5)\n","    train_loss_list.append(loss)\n","\n","    if i%100 == 99:\n","        print(\"epochs : {}, cost : {}\".format(i+1, loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IIV_GsoG2eDs"},"source":["#### 테스트"]},{"cell_type":"code","metadata":{"id":"Dpr0nZhc2Szr"},"source":["print(XOR.predict(X))\n","#0, 1, 1, 0으로 잘 나온다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1IuDL8R7wrx"},"source":["## 다중 클래스 분류 : MNIST Dataset"]},{"cell_type":"markdown","metadata":{"id":"9CiJ5Gmq9Wpa"},"source":["### 배치 처리\n","- 학습 데이터 전체를 한번에 진행하지 않고  \n","  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n","\n","- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n","  미니 배치 학습법(mini-batch learning)이라고도 부름"]},{"cell_type":"markdown","metadata":{"id":"YUDNWwj49byH"},"source":["#### 신경망 구현 : MNIST "]},{"cell_type":"markdown","metadata":{"id":"WjBRQYlP74GM"},"source":["#### 필요한 모듈 임포트"]},{"cell_type":"code","metadata":{"id":"h0lJbkuW71lm"},"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","import tensorflow as tf \n","import time \n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDvtEiD77_gu"},"source":["#### 데이터 로드"]},{"cell_type":"code","metadata":{"id":"4WL7zXMl_uo9"},"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_rNg5Jn8FRA"},"source":["#### 데이터 확인"]},{"cell_type":"code","metadata":{"id":"u4wpsQGA8BOO"},"source":["print(x_train.shape)\n","print(y_train.shape)\n","\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pU7nvkHO8IFR"},"source":["img = x_train[0]\n","print(img.shape)\n","plt.imshow(img, cmap='gray')\n","# 손글씨 5를 넣어주면 얘는 맞춰야 한다."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbBA1Kl18KGT"},"source":["y_train[0]\n","# 얘는 정답이다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTFu8i-z8U_C"},"source":["#### 데이터 전처리 (Data Preprocessing)"]},{"cell_type":"code","metadata":{"id":"q76pjKDVftHJ"},"source":["def flatten_for_mnist(x):\n","    temp = np.zeros((x.shape[0], x[0].size))\n","\n","    for idx, data in enumerate(x):\n","        temp[idx, :] = data.flatten()\n","\n","    return temp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vvMWrDOR8Mns"},"source":["x_train, x_test = x_train / 255.0, x_test / 255.0\n","# 정규화 한다. 0과 1사이의 값으로 모두 나눠준다.\n","x_train = flatten_for_mnist(x_train)\n","x_test = flatten_for_mnist(x_test)\n","\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n","y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n","\n","print(y_train_ohe.shape)\n","print(y_test_ohe.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9LjpWz0dotJs"},"source":["print(x_train[0].max(), x_test[0].min())\n","print(y_train_ohe[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5GUaa92Y9RhY"},"source":["#### 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"code","metadata":{"id":"sk3FXXLi9Th5"},"source":["epochs = 2\n","lr = 0.1\n","batch_size = 100\n","train_size = x_train.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lMJ0h8p8iZl"},"source":["#### 사용되는 함수들(Util Functions)"]},{"cell_type":"code","metadata":{"id":"bSlqZ2Xx8hFn"},"source":["# 대표적인 활성화 함수 리스트\n","\n","def sigmoid(x):\n","    return 1/(1 + np.exp(-x))\n","\n","def mean_squared_error(pred_y, true_y):\n","    return 0.5 * (np.sum((true_y - pred_y)**2))\n","\n","def cross_entropy_error(pred_y, true_y):\n","    if true_y.ndim == 1:\n","        true_y = true_y.reshape(1, -1)\n","        pred_y = pred_y.reshape(1, -1)\n","\n","    delta = 1e-7\n","    return -np.sum(true_y * np.log(pred_y + delta))\n","\n","def cross_entropy_error_for_batch(pred_y, true_y):\n","    if true_y.ndim == 1:\n","        true_y = true_y.reshape(1, -1)\n","        pred_y = pred_y.reshape(1, -1)\n","\n","    delta = 1e-7\n","    batch_size = pred_y.shape[0]\n","\n","    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n","\n","def cross_entropy_for_bin(pred_y, true_y):\n","    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1-pred_y)))\n","\n","def softmax(a):\n","    exp_a = np.exp(a)\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","\n","    return y\n","\n","def differential_1d(f, x):\n","    eps = 1e-5\n","    diff_value = np.zeros_like(x)\n","\n","    for i in range(x.shape[0]):\n","        temp_val = x[i]\n","\n","        x[i] = temp_val + eps\n","        f_h1 = f(x)\n","\n","        x[i] = temp_val - eps\n","        f_h2 = f(x)\n","\n","        diff_value[i] = (f_h1 - f_h2) / (2*eps)\n","        x[i] = temp_val\n","\n","    return diff_value\n","\n","def differential_2d(f, X):\n","    if X.ndim == 1:\n","        return differential_1d(f, X)\n","    else:\n","        grad = np.zeros_like(X)\n","\n","        for idx, x in enumerate(X):\n","            grad[idx] = differential_1d(f, x)\n","        \n","        return grad\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSoV9fyj8_u7"},"source":["#### 2층 신경망으로 구현"]},{"cell_type":"code","metadata":{"id":"XBObD5Fw89HI"},"source":["class MyModel():\n","    def __init__(self):\n","\n","        def weight_init(input_nodes, hidden_nodes, output_units):\n","            np.random.seed(777)\n","\n","            params = {}\n","            params[\"w_1\"] = 0.01 * np.random.randn(input_nodes, hidden_nodes)\n","            params[\"b_1\"] = np.zeros(hidden_nodes)\n","            params[\"w_2\"] = 0.01 * np.random.randn(hidden_nodes, output_units)\n","            params[\"b_2\"] = np.zeros(output_units)\n","            return params\n","\n","        self.params = weight_init(784, 64, 10)\n","\n","    def predict(self, x):\n","        W_1, W_2 = self.params[\"w_1\"], self.params[\"w_2\"]\n","        B_1, B_2 = self.params[\"b_1\"], self.params[\"b_2\"]\n","\n","        A1 = np.dot(x, W_1) + B_1 \n","        Z1 = sigmoid(A1)\n","\n","        A2 = np.dot(Z1, W_2) + B_2\n","        pred_y = softmax(A2)\n","\n","        return pred_y\n","    \n","    def accuracy(self, x, true_y):\n","        pred_y = self.predict(x)\n","        y_argmax = np.argmax(pred_y, axis=1)\n","        t_argmax = np.argmax(true_y, axis=1)\n","\n","        accuracy = np.sum(y_argmax == t_argmax) / float(x.shape[0])\n","\n","        return accuracy\n","\n","    def loss(self, x, true_y):\n","        pred_y = self.predict(x)\n","        return cross_entropy_for_bin(pred_y, true_y)\n","\n","    def get_gradient(self, x, t):\n","        def loss_grad(grad):\n","            return self.loss(x, t)\n","\n","        grads = {}\n","        grads[\"w_1\"] = differential_2d(loss_grad, self.params[\"w_1\"])\n","        grads[\"b_1\"] = differential_2d(loss_grad, self.params[\"b_1\"])\n","        grads[\"w_2\"] = differential_2d(loss_grad, self.params[\"w_2\"])\n","        grads[\"b_2\"] = differential_2d(loss_grad, self.params[\"b_2\"])\n","\n","        return grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"maKNIlK-xJ5k"},"source":["#### 모델 생성 및 학습\n","- 시간 많이 소요"]},{"cell_type":"code","metadata":{"id":"XSEARgNIop8t","colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["30a71f72fcd5467385ad9f5f0ca0c3cc","a5d660bcaec642c7bda85a5993e3472d","14739c2e4170434aac558714163e00b9","3585c82c62d64330bcaa37477d19f14c","f530039c91de4f06a511695e75c3d2bb","0596f0d416624eed801c4b254ab6f98e","d0d169de9e8b4fcc881c95c4f2673462","8a2366e9a836460192cd6660ab8da678","e7232cc20147436b8598993e6e43bfad","14b20fab96634f61b7d874383e692793","029b34ed83894605a093c2333f46cff2"]},"outputId":"acba4b06-21f6-49be-be39-fe578283959d","executionInfo":{"status":"ok","timestamp":1659445755052,"user_tz":-60,"elapsed":55610,"user":{"displayName":"이경윤","userId":"09327853121914493654"}}},"source":["model = MyModel()\n","\n","train_loss_list = list()\n","train_acc_list = list()\n","\n","test_acc_list = list()\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","start_time = time.time()\n","\n","for i in tqdm(range(epochs)):\n","    batch_idx = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_idx]\n","    y_batch = y_train_ohe[batch_idx]\n","\n","    grads = model.get_gradient(x_batch, y_batch)\n","\n","    for key in grads.keys():\n","        model.params[key] -= lr * grads[key]\n","\n","    loss = model.loss(x_batch, y_batch)\n","    train_loss_list.append(loss)\n","\n","    train_accuracy = model.accuracy(x_train, y_train_ohe)\n","    test_accuracy = model.accuracy(x_test, y_test_ohe)\n","    train_acc_list.append(train_accuracy)\n","    test_acc_list.append(test_accuracy)\n","\n","    print(\"epochs : {}, cost : {}, train accuracy : {}, test accuracy : {}\".format(i+1, loss, train_accuracy, test_accuracy))\n","\n","end_time = time.time()\n","\n","print(\"총 학습 소요시간 : {:.3f}s\".format(end_time - start_time))"],"execution_count":null,"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30a71f72fcd5467385ad9f5f0ca0c3cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["epochs : 1, cost : 465.9060502356192, train accuracy : 0.10441666666666667, test accuracy : 0.1028\n","epochs : 2, cost : 361.8440950235913, train accuracy : 0.09751666666666667, test accuracy : 0.0974\n","총 학습 소요시간 : 190.846s\n"]}]},{"cell_type":"markdown","metadata":{"id":"b7nL8f20x4zl"},"source":["### 모델의 결과\n","- 모델은 학습이 잘 될 수도, 잘 안될 수도 있음\n","\n","- 만약, 학습이 잘 되지 않았다면,  \n","  학습이 잘 되기 위해서 어떠한 조치를 취해야 하는가?\n","  - 다양한 학습관련 기술이 존재"]}]}