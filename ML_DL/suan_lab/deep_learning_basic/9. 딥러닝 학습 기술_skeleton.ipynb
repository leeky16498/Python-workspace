{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_9 딥러닝 학습 기술.ipynb의 사본","provenance":[{"file_id":"1zx9k16VII8MZ3_n6VcnsBeiOoQQxavBF","timestamp":1659455485112}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"T7RbRq7bKrSl"},"source":["# 딥러닝 학습 기술\n","\n","- Optimization (매개변수 갱신(확률적 경사하강법, 모멘텀, AdaGrad, Adam) )\n","\n","- Weight Decay\n","- Batch Normalization\n","- 과대적합(Overfitting) / 과소적합(Underfitting)\n","- 규제화(Regularization)\n","- 드롭아웃(Drop Out)\n","- 하이퍼 파라미터\n","  - 학습률(Learning Rate)\n","  - 학습횟수\n","  - 미니배치 크기\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qPY3S6UCKum-"},"source":["## 최적화 방법 : 매개변수 갱신"]},{"cell_type":"markdown","metadata":{"id":"UyKUPpJvKxTs"},"source":["### 확률적 경사하강법(Stochastic Gradient Descent, SGD)\n","\n","  - 전체를 한번에 계산하지 않고, **확률적**으로 일부 샘플을 뽑아 조금씩 나누어 학습을 시키는 과정  \n","  \n","  - 반복할 때마다 다루는 데이터의 수가 적기때문에 한 번 처리하는 속도는 빠름  \n","\n","  - 한 번 학습할 때 필요한 메모리만 있으면 되므로 매우 큰 데이터셋에 대해서도 학습이 가능\n","\n","  - 확률적이기 때문에, 배치 경사하강법보다 불안정\n","\n","  - 손실함수의 최솟값에 이를 때까지 다소 위아래로 요동치면서 이동\n","\n","  - 따라서, 위와 같은 문제 때문에 **미니 배치 경사하강법**(mini-batch gradient descent)로 학습을 진행  \n","    요즘에는 보통 SGD라고하면 미니 배치 경사하강법을 의미하기도 함\n","\n","  \n","  - (참고)\n","    - <code>배치 경사하강법</code>\n","    - <code>미니 배치 경사하강법</code>\n","    - <code>확률적 경사하강법</code>\n","  \n","  <br>\n","\n","  ## $\\quad W \\leftarrow W - \\gamma \\frac{\\partial L}{\\partial W}$  \n","   - $\\gamma :\\ $ 학습률\n","\n","  <br>\n","\n","![](https://engmrk.com/wp-content/uploads/2018/04/Fig2.png)\n","<sub>출처: https://engmrk.com/mini-batch-gd/</sub>\n","\n","\n","  \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"FwUy7mx_KjAb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EF-AMLICK0bL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OWYrwWodK6GJ"},"source":["### SGD의 단점\n","- 단순하지만, 문제에 따라서 시간이 매우 오래걸림"]},{"cell_type":"markdown","metadata":{"id":"7pqcwf0gK9O5"},"source":["### 모멘텀(Momentum)\n","- 운동량을 의미, 관성과 관련\n","\n","- 공이 그릇의 경사면을 따라서 내려가는 듯한 모습\n","\n","- 이전의 속도를 유지하려는 성향  \n","  경사하강을 좀 더 유지하려는 성격을 지님\n","\n","- 단순히 SGD만 사용하는 것보다 적게 방향이 변함\n","  \n","![](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-1-4842-4470-8_33/MediaObjects/463852_1_En_33_Fig1_HTML.jpg)\n","\n","<sub>출처: https://link.springer.com/chapter/10.1007/978-1-4842-4470-8_33</sub>\n","\n","$\n","\\qquad v \\ \\leftarrow \\ \\alpha \\ v - \\gamma \\ \\frac{\\partial L}{\\partial W} \\\\\n","\\qquad W \\ \\leftarrow \\ W \\ + \\ v\n","$\n","\n","  - $\\alpha\\ $ : 관성계수 \n","\n","  - $v$ :  속도\n","\n","  - $\\gamma\\ $ : 학습률\n","\n","  - $\\frac{\\partial L}{\\partial W}\\ $ : 손실함수에 대한 미분\n"]},{"cell_type":"code","metadata":{"id":"9MIfOV6xK16f"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cNr1hKRcLCRR"},"source":["### AdaGrad(Adaptive Gradient)\n","- **가장 가파른 경사를 따라 빠르게 하강하는 방법**\n","\n","- 적응적 학습률이라고도 함\n","  학습률을 **변화시키며 진행**\n","\n","- 경사가 급할 때는 빠르게 변화,  \n","  완만할 때는 느리게 변화\n","\n","- 간단한 문제에서는 좋을 수는 있지만 딥러닝(Deep Learning)에서는 자주 쓰이지 않음\n","  학습률이 너무 감소되어 전역최소값(global minimum)에 도달하기 전에   \n","  학습이 빨리 종료될 수 있기 때문\n","\n","\n","<br>\n","\n","$\n","\\qquad h \\ \\leftarrow \\  h + \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W} \\\\\n","\\qquad W \\ \\leftarrow \\ W \\ + \\gamma \\frac{1}{\\sqrt h} \\ \\frac{\\partial L}{\\partial W}\n","$\n","\n","  <br>\n","\n","  - $h\\ $ : 기존 기울기를 제곱하여 더한 값\n","\n","  - $\\gamma\\ $ : 학습률\n","\n","  - $\\frac{\\partial L}{\\partial W}\\ $ : $W$에 대한 미분\n","\n","  <br>\n","\n","  - (참고)  \n","    - 과거의 기울기를 제곱하여 계속 더해하기 때문에  \n","      학습을 진행할수록 갱신 강도가 약해짐($\\because \\frac{1}{\\sqrt h}$)    \n","  "]},{"cell_type":"code","metadata":{"id":"N4TORymQK-yE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQTD3UUkLJDt"},"source":["### RMSProp (Root Mean Square Propagation)\n","- AdaGrad를 보완하기 위한 방법으로 등장\n","\n","- 합 대신 지수의 평균값을 활용\n","\n","- 학습이 안되기 시작하면 학습률이 커져 잘 되게끔하고,  \n","  학습률이 너무 크면 학습률을 다시 줄임\n","\n","\n","<br>\n","\n","$\n","\\qquad h \\ \\leftarrow \\  \\rho \\ h + (1 - \\rho)\\ \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W} \\\\\n","\\qquad W \\ \\leftarrow \\ W \\ + \\gamma \\frac{\\partial L}{\\partial W} / \\ \\sqrt{h + \\epsilon}\n","$\n","\n","  <br>\n","\n","  - $h\\ $ : 기존 기울기를 제곱하여 업데이트 계수를 곱한 값과 업데이트 계수를 곱한 값을 더해줌 \n","\n","  - $\\rho\\ $ : 지수 평균의 업데이트 계수\n","\n","  - $\\gamma\\ $ : 학습률\n","\n","  - $\\frac{\\partial L}{\\partial W}\\ $ : $W$에 대한 미분"]},{"cell_type":"code","metadata":{"id":"wROsDRxDGC75"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b4d5CeZ_LPaY"},"source":["### Adam (Adaptive moment estimation)\n","- 모멘텀 최적화와 RMSProp의 아이디어를 합친 것\n","\n","- 지난 그래디언트의 지수 감소 평균을 따르고(Momentum), 지난 그레디언트 제곱의 지수 감소된 평균(RMSProp)을 따름\n","\n","- 가장 많이 사용되는 최적화 방법\n","\n","\n","<br>\n","\n","$\n","\\qquad t \\ \\leftarrow \\ t + 1 \\\\\n","\\qquad m_t \\ \\leftarrow \\  \\beta_1 \\ m_{t-1} - (1 - \\beta_1)\\ \\frac{\\partial L}{\\partial W}  \\\\\n","\\qquad v_t \\ \\leftarrow \\ \\beta_2 \\ v_{t-1} + (1 - \\beta_2) \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W} \\\\\n","\\qquad \\hat{m_t} \\ \\leftarrow \\frac{m_t}{1 - \\beta_1^t} \\\\\n","\\qquad \\hat{v_t} \\ \\leftarrow \\frac{v_t}{1 - \\beta_2^t} \\\\\n","\\qquad W_t \\ \\leftarrow \\ W_{t-1} \\ + \\gamma \\ \\hat{m_t}\\  / \\sqrt{\\hat{v_t} + \\epsilon}\n","$\n","\n","  <br>\n","\n","  - $\\beta\\ $ : 지수 평균의 업데이트 계수\n","\n","  - $\\gamma\\ $ : 학습률\n","\n","  - $\\beta_1 \\approx 0.9\\ ,\\ \\ \\beta_2 \\approx 0.999$\n","\n","  - $\\frac{\\partial L}{\\partial W}\\ $ : $W$에 대한 미분"]},{"cell_type":"code","metadata":{"id":"BFyE0iaFLL0M"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rm-SFxGGLYKq"},"source":["## 최적화 방법 비교 (예, Linear Regression)\n","\n","<img src=\"https://user-images.githubusercontent.com/11681225/50016682-39742a80-000d-11e9-81da-ab0406610b9c.gif\" width=\"600\">\n","\n","<sub>출처: https://github.com/ilguyi/optimizers.numpy</sub>"]},{"cell_type":"markdown","metadata":{"id":"8uQ6VOltLaOG"},"source":["## AI 두 번째 위기 (가중치 소실, Gradient Vanishing)\n","\n","- 활성화함수가 Sigmoid 함수 일 때, 은닉층의 갯수가 늘어 날수록  \n","  가중치가 역전파되면서 가중치 소실문제 발생\n","  - 0 ~ 1 사이의 값으로 출력되면서 0 또는 1에 가중치 값이 퍼짐  \n","    이는 <u>미분값이 점점 0에 가까워짐</u>을 의미하기도 한다.\n","\n","  - **ReLU 함수 등장(비선형 함수)**\n","\n","- 가중치 초기화 문제(은닉층의 활성화값 분포)\n","  - 가중치의 값이 일부 값으로 치우치게 되면  \n","    활성화 함수를 통과한 값이 치우치게 되고, 표현할 수 있는 신경망의 수가 적어짐.\n","\n","  - 따라서, 활성화값이 골고루 분포되는 것이 중요!\n","\n","![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F278186%2Fd158ec3585bc1551d9f3a03ae13a3a73%2Fvanishing%20gradient%20problem.png?generation=1574233763365617&alt=media)\n","\n","<sub>출처: https://www.kaggle.com/getting-started/118228</sub>"]},{"cell_type":"markdown","metadata":{"id":"lDoVAV_fLdul"},"source":["## 가중치 초기화\n","\n","* https://www.deeplearning.ai/ai-notes/initialization/"]},{"cell_type":"markdown","metadata":{"id":"rYl_0HikLfQT"},"source":["### 초기값 : 0 (zeros)\n","\n","- 학습이 올바르게 진행되지 않음\n","\n","- 0으로 설정하면  \n","  오차역전파법에서 모든 가중치의 값이 똑같이 갱신됨"]},{"cell_type":"code","metadata":{"id":"BSsNNBbHLeCb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZYtfJU_LgeY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3tX8pl-TLjDE"},"source":["### 초기값 : 균일분포(Uniform)\n","- 활성화 값이 균일하지 않음(활성화함수 : sigmoid)\n","\n","- 역전파로 전해지는 기울기값이 사라짐"]},{"cell_type":"code","metadata":{"id":"ID0hsP-hLhsm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uyVrwCfLkYg"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xtaDIKQOLns3"},"source":["### 초기값 : 정규분포(nomalization)\n","- 활성화함수를 통과하면 양쪽으로 퍼짐\n","\n","- 0과 1에 퍼지면서 기울기 소실문제(gradient vanishing) 발생"]},{"cell_type":"code","metadata":{"id":"ExlCzkHeLloy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cdb94aLtLpGY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NVHTWyeLr2W"},"source":["### 아주 작은 정규분포값으로 가중치 초기화\n","- 0과 1로 퍼치지는 않았고, 한 곳에 치우쳐 짐\n","\n","- 해당 신경망이 표현할 수 있는 문제가 제한됨"]},{"cell_type":"code","metadata":{"id":"WU-yLMRKLqI3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E860A3Y1Ls1I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1j0IEjPLv1g"},"source":["### 초기값 : Xavier (Glorot)\n","- 은닉층의 노드의 수가 n이라면 표준편차가 $\\frac{1}{\\sqrt{n}}$인 분포\n","\n","- 더 많은 가중치에 역전파가 전달 가능하고,  \n","  비교적 많은 문제를 표현할 수 있음\n","\n","- 활성화 함수가 **선형**인 함수일 때 매우 적합"]},{"cell_type":"code","metadata":{"id":"9gHHILm5LuFN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CrZazzpjLyCN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWTHG_CDLzub"},"source":["### 초기값 : Xavier (Glorot) - tanh\n","- 활성화 함수: tanh\n","\n","- sigmoid 함수보다 더 깔끔한 종모양으로 분포  \n"]},{"cell_type":"code","metadata":{"id":"piiqI_3YLxAX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"556nPeAwL081"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ApbdW0yL4D_"},"source":["## 비선형 함수에서의 가중치 초기화"]},{"cell_type":"markdown","metadata":{"id":"V1Y5Mqo5L5VX"},"source":["### 초기값: 0 (Zeros)\n","- 활성화함수: ReLU"]},{"cell_type":"code","metadata":{"id":"n32Jx1RmL2DM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qm6JA1KQL6Y6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sslazMXKL9bP"},"source":["### 초기값: 정규분포(Nomalization)\n","- 활성화함수 : ReLU"]},{"cell_type":"code","metadata":{"id":"GZ23dCR-L7kN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYq5Mu4SL-oj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mR9Jz3_fMCNl"},"source":["#### 표준편차: 0.01 일 때"]},{"cell_type":"code","metadata":{"id":"jyUp8FQLL_08"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8uqyiKdMDeN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dBbPUMmIMHJa"},"source":["### 초기값 : Xavier (Glorot)"]},{"cell_type":"code","metadata":{"id":"Tmnc-DgwMFMs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciwcOOEdMISK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DohDKwbMKvi"},"source":["### 초기값 : He\n","- 표준편차가 $\\sqrt{\\frac{2}{n}}$인 분포\n","\n","- 활성화값 분포가 균일하게 분포되어 있음\n","\n","- 활성화함수가 ReLU와 같은 **비선형**함수 일 때 더 적합하다고 알려진 분포"]},{"cell_type":"code","metadata":{"id":"hCt3RRewMJUY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvZrFwFWMMQD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-46LuSaMU10"},"source":["## 배치 정규화 (Batch Normalization)\n","\n","- 가중치의 활성화값이 적당히 퍼지게끔 '강제'로 적용시키는 것\n","\n","- 미니배치 단위로 데이터의 평균이 0, 표준편차가 1로 정규화\n","\n","- 학습을 빨리 진행할 수 있음\n","\n","- 초기값에 크게 의존하지 않아도 됨\n","\n","- 과적합을 방지\n","\n","- 보통 Fully-Connected와 활성화함수(비선형) 사이에 놓임\n","\n","![](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2018-01-23-at-2.27.20-PM.png)\n","\n","<sub>출처: https://www.jeremyjordan.me/batch-normalization/</sub>"]},{"cell_type":"code","metadata":{"id":"ZVHz_WG0MVJ8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MJmIwMcMXpW"},"source":["## 과대적합(Overfitting) / 과소적합(Underfitting)\n","\n","![](https://miro.medium.com/max/2400/1*UCd6KrmBxpzUpWt3bnoKEA.png)\n","\n","<sub>출처: https://towardsdatascience.com/underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6fe4a8a49dbf</sub>"]},{"cell_type":"markdown","metadata":{"id":"xUQRmlPEMa1w"},"source":["### 과대적합 (Overfitting, 오버피팅)\n","- 모델이 학습 데이터에 한에서만 좋은 성능을 보이고 새로운 데이터에는 그렇지 못한 경우\n","\n","- 학습 데이터가 매우 적을 경우\n","\n","- 모델이 지나치게 복잡한 경우 \n","\n","- 학습 횟수가 매우 많을 경우\n","\n","<br>\n","\n","- 해결방안\n","  - 학습 데이터를 다양하게 수집\n","\n","  - 모델을 단순화\n","    - 파라미터가 적은 모델을 선택하거나, 학습 데이터의 특성 수를 줄이거나\n","  \n","  - 정규화(Regularization)을 통한 규칙을 단순화\n","\n","  - 적정한 하이퍼 파라미터 찾기"]},{"cell_type":"markdown","metadata":{"id":"9Rx8kyOpMcUS"},"source":["### 과소적합 (Underfitting, 언더피팅)\n","- 학습 데이터를 충분히 학습하지 않아 성능이 매우 안 좋을 경우\n","\n","- 모델이 지나치게 단순한 경우\n","\n","<br>\n","\n","- 해결방안\n","  - 충분한 학습 데이터 수집\n","\n","  - 보다 더 복잡한 모델\n","\n","  - 에폭수(epochs)를 늘려 충분히 학습 "]},{"cell_type":"markdown","metadata":{"id":"iC79Q19VMeov"},"source":["## 규제화(Regularization) - 가중치 감소\n","\n","- 과대적합(Overfitting, 오버피팅)을 방지하는 방법 중 하나\n","\n","- 과대적합은 가중치의 매개변수 값이 커서 발생하는 경우가 많음  \n","  이를 방지하기 위해 **큰 가중치 값에 큰 규제를 가하는 것**\n","\n","- 규제란 가중치의 절댓값을 가능한 작게 만드는 것으로,  \n","  가중치의 모든 원소를 0에 가깝게 하여 모든 특성이 출력에 주는 영향을 최소한으로 만드는 것(기울기를 작게 만드는 것)을 의미한다.  \n","  즉, 규제란 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미\n","\n","- 적절한 규제값을 찾는 것이 중요.\n"]},{"cell_type":"markdown","metadata":{"id":"7J-os5aqMg__"},"source":["### L2 규제\n","- 가중치의 제곱합\n","\n","- 손실 함수일정 값을 더함으로써 과적합을 방지\n","\n","- $\\lambda$ 값이 크면 가중치 감소가 커지고,  \n","  작으면 가하는 규제가 적어진다.\n","\n","- 더 Robust한 모델을 생성하므로 L1보다 많이 사용됨\n","\n","<br>\n","\n","## $\\qquad Cost = \\frac{1}{n} \\sum{^n}_{i=1} {L(y_i, \\hat{y_i}) + \\frac{\\lambda}{2}w^2}$\n","\n","### $\\quad L(y_i, \\hat{y_i})$ : 기존 Cost Function"]},{"cell_type":"code","metadata":{"id":"oB3tTCdEMe0u"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_Nekb9aMiCc"},"source":["### L1 규제\n","- 가중치의 절대값합\n","\n","- L2 규제와 달리 어떤 가중치는 0이 되는데 이는 모델이 가벼워짐을 의미\n","\n","<br>\n","\n","## $\\qquad Cost = \\frac{1}{n} \\sum{^n}_{i=1} {L(y_i, \\hat{y_i}) + \\frac{\\lambda}{2}|w|}$\n","\n","### $\\quad L(y_i, \\hat{y_i})$ : 기존 Cost Function"]},{"cell_type":"code","metadata":{"id":"Hh4icLpxMjNx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lpg8N7AXNJpe"},"source":["## 드롭아웃(Dropout)\n","\n","- 과적합을 방지하기 위한 방법\n","\n","- 학습할 때 사용하는 노드의 수를 전체 노드 중에서 **일부만을 사용**\n","\n","- 보통 ratio_value는 0.5 또는 0.7\n","\n","![](https://miro.medium.com/max/981/1*EinUlWw1n8vbcLyT0zx4gw.png)\n","\n","<sub>출처: https://medium.com/konvergen/understanding-dropout-ddb60c9f98aa</sub>"]},{"cell_type":"code","metadata":{"id":"JAydC3fSNJ7V"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RzotyuFjNL79"},"source":["## 하이퍼 파라미터(Hyper Parameter)"]},{"cell_type":"markdown","metadata":{"id":"MovMTKvLNOJR"},"source":["### 학습률(Learning Rate)\n","- 적절한 학습률에 따라 학습정도가 달라짐  \n","  **적당한** 학습률을 찾는 것이 핵심"]},{"cell_type":"markdown","metadata":{"id":"uU4VwrkLNR_8"},"source":["### 학습 횟수(Epochs)\n","- 학습 횟수를 너무 작게, 또는 너무 크게 지정하면  \n","  과소적합 또는 과적합을 띈다.\n","\n","- 몇 번씩 진행하면서 최적의 epochs값을 찾아야한다."]},{"cell_type":"markdown","metadata":{"id":"D0CoUCbKNToW"},"source":["### 미니배치 크기(Mini Batch Size)\n","- 미니 배치 학습\n","  - 한번 학습할 때 메모리의 부족현상을 막기 위해  \n","    전체 데이터의 일부를 여러번 학습하는 방식\n","\n","- 한번 학습할 때마다 얼마만큼의 미니배치 크기를 사용할지 결정\n","\n","- 배치 크기가 작을수록 학습 시간이 많이 소요되고,  \n","  클수록 학습 시간이 학습 시간은 적게 소요된다.  \n","  "]},{"cell_type":"markdown","metadata":{"id":"9jdpc8gdNVt3"},"source":["### 검증데이터(Validation Data)\n","- 주어진 데이터를  \n","  학습 + 검증 + 테스트 데이터로 구분하여 과적합을 방지\n","\n","- 일반적으로 전체 데이터의 2~30%를 테스트 데이터,  \n","  나머지에서 20%정도를 검증용 데이터,  \n","  남은 부분을 학습용 데이터로 사용\n","\n","![](https://miro.medium.com/max/1400/1*4G__SV580CxFj78o9yUXuQ.png)\n","\n","<sub>출처: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6</sub>"]},{"cell_type":"markdown","metadata":{"id":"itBjeGS-vQi-"},"source":["## MNIST 분류"]},{"cell_type":"markdown","metadata":{"id":"sgz8_GjSveBi"},"source":["### Modules Import"]},{"cell_type":"code","metadata":{"id":"5p9ArEUkInYk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dg7-vr1vj1_"},"source":["### 데이터 로드"]},{"cell_type":"code","metadata":{"id":"Vu_5fUTVvfyD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCILIUjovmhj"},"source":["### 데이터 전처리"]},{"cell_type":"code","metadata":{"id":"vZX6J11WvgsD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqckLwhYvgwI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dwwZGRjbtXDy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lXAz-S0Nv0gn"},"source":["### Hyper Parameters"]},{"cell_type":"code","metadata":{"id":"maUj-a-5vg5i"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVKs6vDIwCZN"},"source":["### Util Functions"]},{"cell_type":"code","metadata":{"id":"NVYcEN1pvhWK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mgkGwvZewGE2"},"source":["### Util Classes"]},{"cell_type":"markdown","metadata":{"id":"efZ5-0QIwF7y"},"source":["#### ReLU"]},{"cell_type":"code","metadata":{"id":"oUN_eIfjvhc4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jq_Umt-qwOLx"},"source":["#### Sigmoid"]},{"cell_type":"code","metadata":{"id":"4LdQiHGHvha7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fm_FInuUwQUQ"},"source":["#### Layer"]},{"cell_type":"code","metadata":{"id":"_OgM39CRvhQK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSArAOBAzZ8H"},"source":["#### Batch Normalization"]},{"cell_type":"code","metadata":{"id":"rZwJVo30zbIZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7zcprwHBzBPn"},"source":["#### Dropout\n"]},{"cell_type":"code","metadata":{"id":"tiHwGZQgzCtb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUO5dDGtwa62"},"source":["#### Softmax"]},{"cell_type":"code","metadata":{"id":"Fe4RY7pVvhN5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BIjbtanh10Mt"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"h442ImGrvhMm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pHtNGXBqt4Vp"},"source":["### 모델 생성 및 학습 1\n","- 사용한 기법\n","  - 학습데이터 수 10,000\n","\n","  - Hidden Layers : 4\n","    - [100, 100, 100, 100]\n","  - SGD\n","  - EPOCHS : 1000\n","  - 학습률 : 1e-2 (0.01)\n","  - 배치사이즈 : 256\n","  - 드롭아웃 : 0.2\n","  - 배치 정규화\n","  - 규제화 : 0.1"]},{"cell_type":"code","metadata":{"id":"QUxhUnI4vhHt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDHmqeUl2qPN"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4h1_KYo12qqd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwPofLPxvhFo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q1GqiWjN4ry4"},"source":["#### 시각화"]},{"cell_type":"code","metadata":{"id":"9cl9oGWNvhES"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDFYdkxkvgmb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZmqdHTauXSR"},"source":["### 모델 생성 및 학습 2\n","- 사용한 기법\n","  - 학습데이터 수 10,000\n","\n","  - Hidden Layers = 4\n","    - [100, 100, 100, 100]\n","  - Adam\n","  - EPOCHS : 1000\n","  - 학습률 : 1e-3 (0.001)\n","  - 배치사이즈 : 100\n","  - 드롭아웃 : 0.5\n","  - 배치 정규화\n","  - 규제화 : 0.15"]},{"cell_type":"code","metadata":{"id":"7tO9vazOuVKn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcIkhjZmuY5W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDy32NyNuaY4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhlTsQeOubnX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3oIQ6MRtuodG"},"source":["#### 시각화"]},{"cell_type":"code","metadata":{"id":"r2Z3Mzk6ugQd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3O45zM8uqev"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QKrLVC7utVp"},"source":["### 모델 생성 및 학습 3\n","\n","- 사용한 기법\n","  - 학습데이터 수 20,000\n","\n","  - Hidden Layers = 3\n","    - [256, 100, 100]\n","  - Adam\n","  - EPOCHS : 1000\n","  - 학습률 : 1e-2 (0.01)\n","  - 배치사이즈 : 100\n","  - 배치정규화\n","\n"]},{"cell_type":"code","metadata":{"id":"PSBjZ_VYurqi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0MiK-zKuvEU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o87ClHpIuwK1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TfdWBtgJxP_g"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QQvzV-h2xfDJ"},"source":["#### 시각화"]},{"cell_type":"code","metadata":{"id":"P-ZgzUFoxSQC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMcbhRDjxgcb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Savtn0OxyHal"},"source":["### 3가지 모델 비교\n","- 위의 3가지 모델은 전체적으로 학습 데이터 수를 일부로 제한했기 때문에 학습이 잘 안 될 가능성이 높음  \n","  - 따라서 여러 학습 기술들을 적용함\n"]},{"cell_type":"code","metadata":{"id":"UbHbrNdZxiDw"},"source":[""],"execution_count":null,"outputs":[]}]}